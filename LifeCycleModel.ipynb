{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Today's topic**\n",
        "\n",
        "The aim for today's workshop is to discuss how we use JAX to solve and estimate dynamic life-cycle models by ccp inversion. The JAX API allow e.g. for hardware acceleration and just-in-time (jit) compilation.\n",
        "\n",
        "**Usefull functions**\n",
        "\n",
        "* einsum()\n",
        "* while_loop()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oh96xenKJzHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax._src.api import block_until_ready\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "\n",
        "def rnd(*args):\n",
        "  return random.normal(random.PRNGKey(123), (args))\n",
        "\n",
        "I, J, K, L = 10, 4, 5, 6\n",
        "\n",
        "A, B = rnd(I, J, K), rnd(I, K, L) #create the two ndarrays A and B\n",
        "\n",
        "# Case 1: Vectorice the following calculation\n",
        "C = jnp.empty((I,J,L))\n",
        "for i in range(I):\n",
        "  C = C.at[i,:,:].set(A[i,:,:] @ B[i,:,:])\n",
        "  # C[i,:,:] = A[i,:,:] @ B[i,:,:] #unlike Numpy this is not possible in JAX as arrays are immutable\n",
        "print('shape of C:'+str(C.shape))\n",
        "\n",
        "C_matmul = A @ B\n",
        "print('execution time for matmul: ')\n",
        "%timeit (A @ B).block_until_ready()\n",
        "\n",
        "C_einsum = jnp.einsum('ijk, ikl -> ijl', A, B)\n",
        "print('execution time for einsum: ')\n",
        "%timeit jnp.einsum('ijk, ikl -> ijl', A, B).block_until_ready()\n",
        "\n",
        "print('check whether each element of C and C_matmul are identical: '+str(jnp.all(jnp.isclose(C,C_matmul))))\n",
        "print('check whether each element of C and C_einsum are identical: '+str(jnp.all(jnp.isclose(C,C_einsum))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDI-zmTHuqNg",
        "outputId": "803b7eb0-6207-47c0-eebd-b7dde03692a8"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of C:(10, 4, 6)\n",
            "execution time for matmul: \n",
            "23.1 µs ± 13.9 µs per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n",
            "execution time for einsum: \n",
            "286 µs ± 46.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
            "check whether each element of C and C_matmul are identical: True\n",
            "check whether each element of C and C_einsum are identical: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "I, J, K = 10, 4, 5\n",
        "\n",
        "D, E = rnd(I,K), rnd(J,K,I)\n",
        "\n",
        "# Case 2: Vectorice the following calculation\n",
        "F = jnp.empty((I,J))\n",
        "for i in range(I):\n",
        "  F = F.at[i,:].set(D[i,:] @ E[:,:,i].transpose(1,0) )\n",
        "print('shape of F: '+str(F.shape))\n",
        "\n",
        "F_matmul = jnp.squeeze(jnp.matmul(jnp.reshape(D, (I,1,K)), E.transpose(2, 1, 0)))\n",
        "print('execution time for matmul: ')\n",
        "%timeit jnp.squeeze(jnp.matmul(jnp.reshape(D, (I,1,K)), E.transpose(2, 1, 0))).block_until_ready()\n",
        "\n",
        "F_einsum = jnp.einsum('ik, lki -> il', D, E)\n",
        "print('execution time for einsum: ')\n",
        "%timeit jnp.einsum('ik, lki -> il', D, E).block_until_ready()\n",
        "\n",
        "print('check whether each element of F and F_matmul are identical: '+str(jnp.all(jnp.isclose(F,F_matmul))))\n",
        "print('check whether each element of F and F_einsum are identical: '+str(jnp.all(jnp.isclose(F,F_einsum))))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SlHeJOG3Kat",
        "outputId": "2276ac5f-abe2-421e-b0fe-2daa1fcfc2a2"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of F: (10, 4)\n",
            "execution time for matmul: \n",
            "321 µs ± 13.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
            "execution time for einsum: \n",
            "92.7 µs ± 1.28 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
            "check whether each element of F and F_matmul are identical: True\n",
            "check whether each element of F and F_einsum are identical: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import lax\n",
        "from jax import jit\n",
        "import jax\n",
        "\n",
        "def ArrayAddMult(A,B,C,t):\n",
        "  return C[t+1,:] + A[t,:] @ B[t,:]\n",
        "\n",
        "def SimpleLoop(A,B,C):\n",
        "  T = jnp.shape(A)[0]\n",
        "  for t in reversed(range(T)):\n",
        "    C = C.at[t,:,:].set(ArrayAddMult(A,B,C,t))\n",
        "  return C\n",
        "\n",
        "T, I, J, K, L = 200, 10, 4, 5, 6\n",
        "\n",
        "A, B, C = rnd(T,I,J,K), rnd(T,I,K,L), jnp.zeros((T+1,I,J,L))\n",
        "\n",
        "print('execution time for python for loop: ')\n",
        "%timeit SimpleLoop(A,B,C).block_until_ready()\n",
        "print('execution time for JIT compiled python for loop: ')\n",
        "%timeit jit(SimpleLoop)(A,B,C).block_until_ready()\n",
        "\n",
        "def Cond(tup): # tup = (A, B, C, t)\n",
        "  return (tup[-1] - 1 >= 0)\n",
        "\n",
        "def LaxLoop(tup):\n",
        "  (A, B, C, t) = tup\n",
        "  t = t - 1\n",
        "  C = C.at[t,:,:].set(ArrayAddMult(A,B,C,t))\n",
        "  return (A, B, C, t)\n",
        "\n",
        "tup = (A, B, C, T)\n",
        "print('execution time for lax.while_loop(): ')\n",
        "%timeit jax.block_until_ready(lax.while_loop(cond_fun=Cond, body_fun=LaxLoop, init_val=tup))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlA4NAN6F98K",
        "outputId": "93ff65de-7133-48f7-cc13-637463334d5d"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "execution time for python for loop: \n",
            "1.79 s ± 241 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
            "execution time for JIT compiled python for loop: \n",
            "6.5 ms ± 937 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
            "execution time for lax.while_loop(): \n",
            "1.9 ms ± 395 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Life-cycle model**\n",
        "\n",
        "Let's consider a simple human capital model, where the agents have a finitie time horizon and in each time period must decide on their  labor supply, $\\ell_{it}$, in order to maximizer their expected utility over their life-time\n",
        "\n",
        "\\begin{align}\n",
        "\\max_{\\ell_{it}\\in(0,1)} v_{it}(\\ell_{it},h_{it}) + \\epsilon_{it}(\\ell_{it}),\n",
        "\\end{align}\n",
        "subject to the law of motion for human capital, $h_{it}$, which is governed by the transition probabilities, $p(h_{it+1}|\\ell_{it},h_{it})$.\n",
        "\n",
        "$v_{it}(\\ell_{it},h_{it})$ is the choice-specific value function\n",
        "\\begin{align}\n",
        "v_{it}(\\ell_{it},h_{it}) &= u(\\ell_{it}h_{it}) + \\beta E_{t}V_{it+1}(\\ell_{it},h_{it}),\\\\\n",
        "E_{t}V_{it+1}(\\ell_{it},h_{it}) &= \\sum_{h_{t+1}}p(h_{it+1}|\\ell_{it},h_{it})E_{t+1}V_{it+1}(h_{it+1}),\n",
        "\\end{align}\n",
        "\n",
        "$u(\\ell_{it}h_{it})$ is the instantanous utility, $p(h_{it+1}|\\ell_{it},h_{it})$ is the transition probability for human capital, and $E_{t+1}V_{it+1}(h_{it+1})$ is the expected value function of next period. The instantanous utility is given by sum of utility of consumption, (dis-)utility of working, and the structural error term\n",
        "\n",
        "\\begin{align}\n",
        "u(0,h_{it}) &= u_c(b), \\\\\n",
        "u(1,h_{it}) &= u_c(w(h_{it})) + \\psi_0 + \\psi_1 t + e_{t}(h_{it}).\n",
        "\\end{align}\n",
        "\n",
        "As the saving motive is ignored, the agent simply consume her entire income in each period, and utility of consumption is assumed to given by the CRRA utility function.\n",
        "\n",
        "\\begin{align}\n",
        "u_c(c) &= \\tfrac{c^{1-\\gamma}-1}{1-\\gamma}.\n",
        "\\end{align}\n",
        "When unemployed the agent receive an exogenous benefit, $b$. In contrast, the agent earn a wage income when employed, which is assumed to be a function of the agent's human capital, $w(h_{it})$ Finally, as we will assume the tast-shock, $\\epsilon_{it}(\\ell_{it})$, is extreme value type-I distributed, the choice probabilities and expected utility are given by the well known logit choice probabilities and log-sum\n",
        "\n",
        "\\begin{align}\n",
        "q_{it}(\\ell_{it},h_{it}) &= \\tfrac{\\exp\\{v_{it}(\\ell_{it},h_{it})\\}}{\\exp\\{v_{it}(0,h_{it})\\} + \\exp\\{v_{it}(1,h_{it})\\}}, \\tag{1} \\\\\n",
        "E_{t}V_{it}(h_{it}) &= \\log[\\exp\\{v_{it}(0,h_{it})\\} + \\exp\\{v_{it}(1,h_{it})\\}]. \\tag{2}\n",
        "\\end{align}\n",
        "\n",
        "**CCP inversion**\n",
        "\n",
        "Assume we know the none-linear structural parameters ($\\gamma$,$\\beta$). By inverting the choice probabilities we can isolate the linear utility terms.\n",
        "\\begin{equation}\n",
        " \\log q_t(1,h_{it}) - \\log q_t(0,h_{it}) - \\big\\{u_c(w(h_{it})) - u_c(b) \\big\\} - \\beta \\big\\{ E_{t+1}V_{it+1}(1,h_{it}) - E_{t+1}V_{it+1}(0,h_{it}) \\big\\}  = \\psi_0 + \\psi_1 t + e_t(h_{it}). \\tag{3}\n",
        "\\end{equation}\n",
        "However, in order to evaluate $y_{it}$ we need to calculate the the expected value function, $E_{t+1}V_{it+1}(\\ell_{it},h_{it})$. We can use the trick that the log of the denominator of the choice probabilities equals the expected value, see equation $(1)$-$(2)$\n",
        "\\begin{align}\n",
        "E_{t+1}V_{it+1}(h_{it+1}) &= v(0,h_{it+1}) - \\log q_{it+1}(0,h_{it+1}),\\\\\n",
        "&= \\big\\{ u_c(b) + \\beta \\sum_{h_{t+2}}p(h_{it+2}|0,h_{it+1})E_{t+1}V_{it+2}(h_{it+2}) \\big\\} - \\log q_{it+1}(0,h_{it+1}). \\tag{4}\n",
        "\\end{align}\n",
        "Hence, $E_{t+1}V_{it+1}(\\ell_{it},h_{it})$ and $E_{t+1}V_{it+1}(h_{it+1})$ can be calculated by backward induction.\n",
        "\n",
        "Based on equation $(3)$-$(4)$ we can then estimate the linear structural parameters ($\\psi_0$,$\\psi_1$) by a simple regression\n",
        "\\begin{align}\n",
        " y_{it}(h_{it})  = X_{it} \\psi + e_t(h_{it}), \\tag{5}\n",
        "\\end{align}\n",
        "where $X_{it}$ contains an intercept and the age of the agent (1,t) and $\\psi$ contains the linear parameters $(\\psi_0,\\psi_1)$. Finally, $y_{it}$ is the left side of equation $(5)$."
      ],
      "metadata": {
        "id": "3SwAeReEunv6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "b0BM3dnf71xY"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import lax\n",
        "from jax import random\n",
        "\n",
        "from jax.config import config\n",
        "config.update(\"jax_enable_x64\", True)\n",
        "\n",
        "import dataclasses as dc\n",
        "import statsmodels.api as sm\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dc.dataclass\n",
        "class Par:\n",
        "    # Number of alternatives\n",
        "    L: int = 2 #number of alternatives for the labor supply decision (unemployement, employement)\n",
        "\n",
        "    # Set dimensions of state variables\n",
        "    T: int = 25 #number of time periods the agents live\n",
        "    H: int = T #number of grid points for human capital\n",
        "    TH: int = T*H #combinations of state variables\n",
        "\n",
        "    # Set structural parameters\n",
        "    beta: float = 0.90 #discount factor\n",
        "    gamma: float = 2.00 #CRRA parameter\n",
        "    psi0: float =-0.50 #(dis-)utility from working (baseline)\n",
        "    psi1: float =-0.20 #(dis-)utility from working (linear in age)#\n",
        "\n",
        "    # Set parameters governing the income process\n",
        "    benefit: float = 0.10 #benefit level (income when unemployed)\n",
        "    wageBase: float = 0.10 #wage when human capital is zero\n",
        "    wageGrowth: float = 2.00**(1/(H-1)) #wage growth as human capital increase\n",
        "\n",
        "    # Set parameters governing the distribution of structural errors\n",
        "    std = 0.5\n",
        "\n",
        "par = Par() #set model parameters\n",
        "\n",
        "def ExogenousVariables(par):\n",
        "  \"\"\" Generate exogenous variables of the life-cycle model\n",
        "\n",
        "      Inputs\n",
        "        - par: data class containing model parameters\n",
        "\n",
        "      Outputs\n",
        "        - exog: data class containing the exogenout variables of the model\n",
        "  \"\"\"\n",
        "  # Set income process\n",
        "  income = jnp.empty((par.L,par.H))\n",
        "  income = income.at[0,:].set( par.benefit )\n",
        "  income = income.at[1,:].set( par.wageBase*(par.wageGrowth**jnp.arange(par.H)) )\n",
        "\n",
        "  # Transition probabilities for human capital (HC)\n",
        "  prob_HC = jnp.zeros((par.L,par.H,par.H))\n",
        "  prob_HC = prob_HC.at[0,:,:].set( jnp.eye(par.H) )\n",
        "  prob_HC = prob_HC.at[1,:-1,1:].set( jnp.eye(par.H-1) )\n",
        "  prob_HC = prob_HC.at[1,-1,-1].set( 1.0 )\n",
        "\n",
        "  # Draw structural errors for employment from the normal distribution\n",
        "  error = jnp.zeros((par.L,par.T,par.H))\n",
        "  error = error.at[1,:,:].set(par.std * random.normal(random.PRNGKey(345), shape=(par.T,par.H) ) )\n",
        "\n",
        "  # Store exogenous variables in data class\n",
        "  @dc.dataclass\n",
        "  class Exog:\n",
        "    income: jnp.ndarray #Income process\n",
        "    prob_HC: jnp.ndarray #Transition probabilities for human capital\n",
        "    error: jnp.ndarray #Structural errors\n",
        "\n",
        "  exog = Exog(income=income, prob_HC=prob_HC, error=error)\n",
        "  return exog\n",
        "\n",
        "exog = ExogenousVariables(par)\n",
        "\n",
        "if par.H<10:\n",
        "  print('income when unemployed as a function of human capital, b:')\n",
        "  print(round(exog.income[0,:],2))\n",
        "  print('income when employed as a function of human capital, w(h_t):')\n",
        "  print(round(exog.income[1,:],2))\n",
        "  print('transition probabilities for human capital when unemployed, p(h_t+1|0,h_t):')\n",
        "  print(exog.prob_HC[0,:])\n",
        "  print('transition probabilities for human capital when employed, p(h_t+1|1,h_t):')\n",
        "  print(exog.prob_HC[1,:])\n",
        "  print('structural error as a function of age and human capital, e(h_t):')\n",
        "  print(exog.error[1,:])"
      ],
      "metadata": {
        "id": "IGqiJx0B8Pxt"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Logit(V,axis=0):\n",
        "    \"\"\" Returns the logit choice probabilities and the log-sum of the associated payoff matrix, V \"\"\"\n",
        "    maxV = jnp.max(V, axis=axis, keepdims=True) #used for centering\n",
        "\n",
        "    nominator = jnp.exp(V - maxV) #nominator of the logit choice probabilities\n",
        "    denominator = jnp.sum(nominator, axis=axis, keepdims=True) #denominator of the logit choice probabilities\n",
        "    return nominator / denominator, jnp.log(denominator) + maxV\n",
        "\n",
        "def CRRA(x,gamma):\n",
        "  \"\"\" Return the CRRA utility of consumption, x, given the parameter gamma \"\"\"\n",
        "  return (x**(1.0 - gamma) - 1.0)/(1.0 - gamma)\n",
        "\n",
        "def Utility(income,error,age,gamma,psi0,psi1):\n",
        "  \"\"\" Return the instantanous choice-specific utility function \"\"\"\n",
        "  laborSupply = jnp.arange(income.shape[0])[:,jnp.newaxis] #indicator function for unemployment and employment\n",
        "  return CRRA(income,gamma) + laborSupply * (psi0 + psi1 * age) + error #instantanous choice-specific utilities\n",
        "\n",
        "def Bellman(utility,prob_HC,EVnext,beta):\n",
        "  \"\"\" Solve the Bellman equation\n",
        "\n",
        "      Inputs\n",
        "        - utility: instantanous choice-specific utility\n",
        "        - prob_HC: transition probabilities for human capital\n",
        "        - EVnext: expected value function of next period\n",
        "        - beta: discount factor\n",
        "\n",
        "      Outputs\n",
        "        - v: choice-specific value function\n",
        "        - q: choice probabilities\n",
        "        - EV: expected value function of the period\n",
        "  \"\"\"\n",
        "  # explanation of the subscripts for jnp.einsum()\n",
        "  #   l: labor supply alternatives\n",
        "  #   h: current human capital level\n",
        "  #   k: next period human capital level\n",
        "  v = utility + beta * jnp.einsum('lhk, k -> lh', prob_HC, EVnext) #calculate choice-specific value functions\n",
        "\n",
        "  q, EV = Logit(v) #calculate choice probabilities and the expected value function\n",
        "  return v, q, EV"
      ],
      "metadata": {
        "id": "4ujezu6M8QWw"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Condition(inputTuple):\n",
        "  \"\"\" Stopping criterium for the while loop \"\"\"\n",
        "  t = inputTuple[-1] #unpack t from the tuple (t has to be the last element of the tuple)\n",
        "  return (t - 1 >= 0) #continue if t - 1 > 0\n",
        "\n",
        "def BackwardRecursion(endogTuple,exog,par):\n",
        "  \"\"\" Solve and store the solution of the Bellman equation for the time period t\n",
        "\n",
        "      Inputs\n",
        "        - endogTuple: tuple containing the endogenous variables of the model, (v, q, EV, t)\n",
        "        - exog: data class containing the exogenous variables of the model, (prob_HC, income)\n",
        "        - par: data class containing the structural parameters of the model\n",
        "\n",
        "      Outputs\n",
        "        - tuple containing the updated endogenous variables of the model\n",
        "  \"\"\"\n",
        "  (v, q, EV, t) = endogTuple #unpack tuple\n",
        "\n",
        "  t = t - 1 #recurse backward\n",
        "\n",
        "  u_t = Utility(exog.income,exog.error[:,t,:],t,par.gamma,par.psi0,par.psi1) #calculate instantanous utility\n",
        "  v_t, q_t, EV_t = Bellman(u_t,exog.prob_HC,EV[t+1,:],par.beta) #solve bellman\n",
        "\n",
        "  # store solution for period t\n",
        "  v = v.at[:,t,:].set( v_t )\n",
        "  q = q.at[:,t,:].set( q_t )\n",
        "  EV = EV.at[t,:].set( jnp.squeeze(EV_t) )\n",
        "  return (v, q, EV, t) #return tuple with updated values\n",
        "\n",
        "def SolveLifeCycleModel(exog,par):\n",
        "  \"\"\" Solve the life cycle model by backward induction\n",
        "\n",
        "      Inputs\n",
        "        - exog: data class containing the exogenous variables of the model\n",
        "        - par: data class containing the structural parameters of the model\n",
        "\n",
        "      Outputs\n",
        "        - endog: data class containing the endogenous variables of the model\n",
        "  \"\"\"\n",
        "  v = jnp.empty((par.L,par.T,par.H)) #initialize choice-specific value function\n",
        "  q = jnp.empty((par.L,par.T,par.H)) #initialize choice probabilities\n",
        "  EV = jnp.empty((par.T+1,par.H)) #initialize expected value function\n",
        "\n",
        "  t = par.T #initialize time\n",
        "\n",
        "  endogTuple = (v, q, EV, t) #tuple with endogenous variables\n",
        "\n",
        "  # Solve model by backward induction\n",
        "  Fun = lambda x: BackwardRecursion(x,exog,par)\n",
        "  (v, q, EV, t) = lax.while_loop(body_fun=Fun, cond_fun=Condition, init_val=endogTuple)\n",
        "\n",
        "  # for t in reversed(range(par.T)):\n",
        "  #   endogTuple = Fun(endogTuple)\n",
        "  # (v, q, EV, t) = endogTuple\n",
        "\n",
        "  # Store solution in a data class\n",
        "  @dc.dataclass\n",
        "  class Endog:\n",
        "    v: jnp.ndarray #choice-specific value function\n",
        "    q: jnp.ndarray #choice probabilities\n",
        "    EV: jnp.ndarray #expected value function\n",
        "\n",
        "  endog = Endog(v=v, q=q, EV=EV) #store solution in the data class\n",
        "  return endog"
      ],
      "metadata": {
        "id": "5WvcfLDm8QYn"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MyOLS(y,X):\n",
        "    \"\"\" Estimate linear parameters by ordinary least squares (OLS) \"\"\"\n",
        "    return jnp.linalg.solve(jnp.matmul(X.T, X), jnp.matmul(X.T, y))\n",
        "\n",
        "def BackwardRecursionCCP(evTuple,data,par):\n",
        "  \"\"\" Calculate expected value function by CCP inversion\n",
        "\n",
        "      Inputs\n",
        "        - evTuple: tuple containing (EV, t)\n",
        "        - data: data class containing observed data\n",
        "        - par: data class containing model parameters\n",
        "\n",
        "      Outputs\n",
        "        - evTuple: tuple containing updated values of (EV, t)\n",
        "  \"\"\"\n",
        "  (EV, t) = evTuple #unpack tuple\n",
        "\n",
        "  t = t - 1 #recurse backward\n",
        "\n",
        "  # Calculate expected value functions by CCP inversion\n",
        "  EV_t = data.uC[0,:] - jnp.log(data.q[0,t,:]) + par.beta * EV[t+1,:]\n",
        "\n",
        "  # Store solution\n",
        "  EV = EV.at[t,:].set( EV_t )\n",
        "  return (EV, t) #return tuple with updated values\n",
        "\n",
        "def SetupRegression(data,par):\n",
        "  \"\"\" CCP inversion\n",
        "\n",
        "      Inputs\n",
        "        - data: data class containing observed data\n",
        "        - par: data class containing model parameters\n",
        "\n",
        "      Outputs\n",
        "        - y: vector containing the dependent variable\n",
        "        - X: matrix containing the independent variables\n",
        "  \"\"\"\n",
        "  evTuple = (jnp.zeros((par.T+1,par.H)), par.T) #initialize tuple, evTuple = (EV, t)\n",
        "\n",
        "  data.uC = CRRA(data.income, par.gamma) #utility of consumption\n",
        "\n",
        "  Fun = lambda x: BackwardRecursionCCP(x,data,par)\n",
        "  (EV, t) = lax.while_loop(body_fun=Fun, cond_fun=Condition, init_val=evTuple) #calculate expected value function by CCP inversion\n",
        "\n",
        "  # explanation of the subscripts for jnp.einsum()\n",
        "  #   l: labor supply alternatives\n",
        "  #   h: current human capital level\n",
        "  #   k: next period human capital level\n",
        "  #   t: current time period (current age)\n",
        "  pEV = jnp.einsum('lhk, tk -> lth',data.prob_HC, EV) #calculate choice-specific expected value functions\n",
        "\n",
        "  # Set up dependent variable and independent variables (y, X)\n",
        "  y = jnp.reshape(jnp.log(data.q[1,:]) - jnp.log(data.q[0,:])\n",
        "                  - (data.uC[1,:] - data.uC[0,:])\n",
        "                  - par.beta * (pEV[1,1:,:] - pEV[0,1:,:]), (par.TH,1), order='F' )\n",
        "\n",
        "  X = jnp.c_[jnp.ones((par.TH,1)), jnp.tile(jnp.arange(par.T), (par.H,) ) ]\n",
        "  return y, X\n",
        "\n",
        "def Estimation(data,par):\n",
        "  \"\"\" Estimate linear parameters by CCP inversion\n",
        "\n",
        "      Inputs\n",
        "        - data: data class containing observed data\n",
        "        - par: data class containing model parameters\n",
        "\n",
        "      Outputs\n",
        "        - pvec: vector of estimated linear parameters\n",
        "  \"\"\"\n",
        "  y, X = SetupRegression(data,par) #set up variables for regression based on observed data\n",
        "  #results = MyOLS(y, x)\n",
        "\n",
        "  results = sm.OLS(np.array(y), np.array(X)).fit() #estimate linear model by OLS\n",
        "\n",
        "  return results, y, X"
      ],
      "metadata": {
        "id": "LbzUd2o1q3Bs"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "endog = SolveLifeCycleModel(exog,par) #solve the model to obtain the observed choice probabilities\n",
        "\n",
        "#store the observed data in data class\n",
        "@dc.dataclass\n",
        "class Data:\n",
        "  income: jnp.ndarray #Observed income process\n",
        "  prob_HC: jnp.ndarray #Observed transition probabilities for human capital\n",
        "  q: jnp.ndarray #Observed choice probabilities\n",
        "\n",
        "data = Data(income=exog.income, prob_HC=exog.prob_HC, q=endog.q) #observed data\n",
        "\n",
        "results = Estimation(data,par)[0] #estimates linear parameters (psi0,psi1)\n",
        "\n",
        "# The estimates differ from the true parameter values due to the structural errors\n",
        "print(results.summary())\n",
        "#Note that we have assumed that the none-linear parameters (beta,gamma) are known"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLTXB-xQdF1e",
        "outputId": "2f5341a1-1b54-419d-a770-510a68876817"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                      y   R-squared:                       0.898\n",
            "Model:                            OLS   Adj. R-squared:                  0.897\n",
            "Method:                 Least Squares   F-statistic:                     5463.\n",
            "Date:                Fri, 15 Sep 2023   Prob (F-statistic):          1.54e-310\n",
            "Time:                        02:52:00   Log-Likelihood:                -452.51\n",
            "No. Observations:                 625   AIC:                             909.0\n",
            "Df Residuals:                     623   BIC:                             917.9\n",
            "Df Model:                           1                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const         -0.4387      0.039    -11.300      0.000      -0.515      -0.362\n",
            "x1            -0.2050      0.003    -73.913      0.000      -0.210      -0.200\n",
            "==============================================================================\n",
            "Omnibus:                        0.247   Durbin-Watson:                   1.932\n",
            "Prob(Omnibus):                  0.884   Jarque-Bera (JB):                0.313\n",
            "Skew:                           0.044   Prob(JB):                        0.855\n",
            "Kurtosis:                       2.933   Cond. No.                         27.3\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    }
  ]
}